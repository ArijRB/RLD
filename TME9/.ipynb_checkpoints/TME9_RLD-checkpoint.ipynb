{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O93rKzOfY20t"
   },
   "source": [
    "# <center>M2 DAC -   Reinforcement Learning & Advanced Deep</center>\n",
    "##  <center> TME 9. Generative Adversarial Networks  </center>\n",
    "\n",
    "Ce TME a pour objectif d'expérimenter les Generative Adversarial Networks (GANs) sur un problème de génération de visages. \n",
    "\n",
    "De manière classique, un GAN se formule selon un problème adverse de la manière suivante: \n",
    "$$\\min\\limits_{G} \\max\\limits_D V(D,G) = \\mathbb{E}_{x\\sim p_{data}(x)}\\big[logD(x)\\big] + \\mathbb{E}_{z\\sim p_{z}(z)}\\big[log(1-D(G(z)))\\big]$$\n",
    "Cette formulation met en jeu deux réseaux adverse: \n",
    "*   Un réseau discriminateur $D$, dont l'objectif est de savoir distinguer les données réelles des données simulées  \n",
    "*   Un réseau générateur $G$, dont l'objectif est de flouer le discriminateur\n",
    "\n",
    "À l'optimum, avec des réseaux de capacité infinie, la distribution $p_G$ des données générées par $G$ est prouvée suivre la distribution des données réelles $p_{data}$. Bien sûr nous ne travaillons pas avec des réseaux de capacité infinie (et d'ailleurs heureusement car on ne veut pas apprendre par coeur les données d'apprentissage), mais l'objectif est d'approcher cette distribution $p_{data}$ en apprenant un générateur neuronal dont les sorties sont difficilement distinguables des vraies données pour le discriminateur. \n",
    "\n",
    "Nous proposons de mettre ce genre d'architecture pour un la génération de visages: selon un ensemble de visages d'entraînement, il s'agit d'apprendre à générer des visages qui paraissent les plus réalistes possibles tout en conservant une certaine diversité dans les distributions de sortie. Pour cela nous emploierons une architecture DCGAN, qui utilise des réseaux de neurones convolutionnels (CNNs) pour le générateur et le discriminateur.    \n",
    "\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6no2GQP0eEbL"
   },
   "source": [
    "# Nouvelle section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SVOye36wUyRr"
   },
   "source": [
    "Pour ce TP, nous vous proposons de travailler avec Google Colab qui est un service notebook en ligne avec ressources machines à disposition (nécessite d'être connecté à un compte google personnel):\n",
    "*   https://colab.research.google.com/\n",
    "*   Ouvrir ce Notebook avec \"File>Upload Notebook\"\n",
    "*   Sélectionner un Runtime GPU : Runtime>Change Runtime Type (utile de le faire avant tout téléchargement de données car le changement de Runtime efface les données temporaires de la session)\n",
    "\n",
    "Pour télécharger les données utiles au TP: \n",
    "\n",
    "\n",
    "*   Aller à l'adresse : https://drive.google.com/open?id=0B7EVK8r0v71pWEZsZE9oNnFzTm8\n",
    "*   Clic-droit sur CelebA, Ajouter à mon Drive\n",
    "*   Exécuter le code ci-dessous (une autorisation vous sera demandée, suivre le lien donné à l'exécution pour obtenir le code d'authentification)\n",
    "\n",
    "(Sinon vous pouvez aussi travailler en local en téléchargeant les données sur votre ordinateur mais attention fichier volumineux et colab propose des ressources GPU utiles à la résolution du TP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "GyBTFov1RsD4",
    "outputId": "3f90898e-72da-4456-de1b-aad957766f3b"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f0w7dABTcPpD"
   },
   "source": [
    "Vous devez maintenant avoir dans le paneau de gauche un onglet Files contenant un répertoire drive dans l'arborescence. C'est votre espace de stockage Google Drive. Il devrait contenir un répertoire CelebA (si ce n'est pas le cas attendre un peu et rafraichir, il peut mettre un certain temps à apparaître). \n",
    "\n",
    "Il s'agit maintenant de décompresser l'archive téléchargée (cela peut prendre jusqu'à une dizaine de minutes): \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "xlNkozL5c3LF",
    "outputId": "3617141d-b6e7-4df3-e705-4b28a22dca75"
   },
   "outputs": [],
   "source": [
    "!unzip drive/My\\ Drive/CelebA/Img/img_align_celeba -d data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-WsqUKwHhBWJ"
   },
   "source": [
    "Cela produit un répertoire data à la racine du repertoire temporaire de votre Google Colab. Ce repertoire contient un sous-repertoire img_align_celeba contenant une liste de 202599 fichiers jpg correspondant à des photos de visages de célébrités (attention ce repertoire est temporaire, tout ce qui n'est pas dans un drive est effacé à la fin de la session). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NC19uHxEQzhg"
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "try:\n",
    "    import torch\n",
    "except:\n",
    "    !pip3 install torch==1.2.0+cu92 torchvision==0.4.0+cu92 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "    import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.utils as vutils\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RiS5NSCjj1N2"
   },
   "source": [
    "Le code ci-dessous permet de déclarer la manière de charger les données. \n",
    "\n",
    "Lorsque des données sont demandées (pour la construction d'un nouveau batch par exemple), une série de transformations est appliquée sur les images, selon la composition de transformateurs déclarée pour le chargement: \n",
    "*    redimentionnement des images en 64 par 64\n",
    "*    recadrage au centre (qui ne fait rien ici car image déjà dans la taille du cadre mais si utile pour d'autres paramètres)\n",
    "*    conversion en tenseur pytorch \n",
    "*    normalisation des valeurs RGB selon une moyenne de 0.5 et un ecart-type de 0.5.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IfkUPrmhi7EE"
   },
   "outputs": [],
   "source": [
    "image_size = 64\n",
    "dataset = dset.ImageFolder(root=\"data\",\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize(image_size),\n",
    "                               transforms.CenterCrop(image_size),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "vRoYyNTSjs5e",
    "outputId": "1364c096-6d69-4f26-a08b-d8c32c4b72cd"
   },
   "outputs": [],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cpHar5v3mPo9"
   },
   "source": [
    "Le code ci-dessous permet de déclarer la manière de charger les images et en affiche un échantillon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 862
    },
    "colab_type": "code",
    "id": "Alkf-VTNmNrz",
    "outputId": "6686bd75-39dd-47ef-8a7b-47f9aebf9546"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "seed=0\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "device=0\n",
    "if device>=0 and torch.cuda.is_available():\n",
    "  cudnn.benchmark = True\n",
    "  torch.cuda.device(device)\n",
    "  torch.cuda.manual_seed(seed)\n",
    "else: \n",
    "  device=-1\n",
    "\n",
    "batch_size = 128\n",
    "workers = 2\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=workers)\n",
    "\n",
    "# Affichage de quelques images\n",
    "real_batch = next(iter(dataloader)) #real_batch est une liste de 2 tenseurs où le 1er correspond aux images, les second correspond aux labels (ici 0 partout)\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Training Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=1, normalize=True).cpu(),(1,2,0)))\n",
    "os.makedirs(\"drive/My Drive/genFaces\",exist_ok=True)\n",
    "plt.savefig(\"drive/My Drive/genFaces/train.png\" ) # Pour sauvegarder l'image sur votre Google Drive \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8ssBMv7C1HL7"
   },
   "source": [
    "Le réseau $D$ est un empilement de couches de convolution 2D avec batchNorm2D et activations RELU: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "colab_type": "code",
    "id": "cn-rGKpEaYtE",
    "outputId": "826a55c6-d125-4e4e-9d88-5b6cdf6b8492"
   },
   "outputs": [],
   "source": [
    "\n",
    "nc = 3 # Nombre de canaux de l'entrée\n",
    "ndf = 64 # Facteur du nombre de canaux de sortie des différentes couches de convolution\n",
    "\n",
    "\n",
    "# Initialisation recommandee pour netG et netD dans DCGAN\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "\n",
    "netD = Discriminator().to(device)\n",
    "netD.apply(weights_init)\n",
    "print(netD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "colab_type": "code",
    "id": "1jP0pKRVBKAt",
    "outputId": "955f4839-7de4-4bbd-8b4c-3130fc1b3c7b"
   },
   "outputs": [],
   "source": [
    "nz=100  #Taille du vecteur z donné en entrée du générateur\n",
    "ngf = 64 # Facteur du nombre de canaux de sortie des différentes couches de deconvolution\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "netG = Generator().to(device)\n",
    "netG.apply(weights_init)\n",
    "print(netG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lh-lapJyhoRd"
   },
   "outputs": [],
   "source": [
    "## Data loading\n",
    "\n",
    "workers = 4 # Number of workers for dataloader (/!\\ set to 4 when you're done debugging)\n",
    "\n",
    "## Architecture\n",
    "\n",
    "nz = 100 # Size of z latent vector (i.e. size of generator input)\n",
    "# nz = 10\n",
    "# nz = 1000\n",
    "ndf = 32 # Base size of feature maps in discriminator\n",
    "ngf = 32 # Base size of feature maps in generator\n",
    "\n",
    "## Optimization\n",
    "\n",
    "lrD = 0.0002 # Learning rate for the discriminator\n",
    "lrG = 0.0002 # Learning rate for the generator\n",
    "beta1G = 0.5 # Momentum beta1 for the discriminator\n",
    "beta1D = 0.5 # Momentum beta1 for the generator\n",
    "\n",
    "## Training\n",
    "\n",
    "batch_size = 256 # Images per batch\n",
    "nb_update_D = 1 # Number of sub-steps of discriminator optim. at each step\n",
    "# nb_update_D = 10\n",
    "nb_update_G = 1 # Number of sub-steps of generator optim. at each step\n",
    "# nb_update_G = 10\n",
    "steps = 8000 # Number of global steps in the training loop\n",
    "nb_epochs = None # Number of epochs, leave \"None\" if you want to set the number of \"steps\" (i.e. batches)\n",
    "# nb_epochs = 30 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gz6Eaa0zhUVT"
   },
   "outputs": [],
   "source": [
    "# Prior P(z). Returns a Gaussian random tensor of shape (batch_size, nz, 1, 1)\n",
    "def get_noise(batch_size):\n",
    "    normal_distrib = torch.distributions.Normal(torch.tensor([0.0]), torch.tensor([1.0]))\n",
    "    noise = normal_distrib.sample((batch_size,nz,1))\n",
    "    return noise.to(device)\n",
    "\n",
    "# Create the criterion function that will take (y_hat, y) as input\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "# Setup Adam optimizers for D and G\n",
    "# optimizerD = torch.optim.Adam(netD.parameters(), lr=lrD, betas=(beta1D,0.999)) # take netD.parameters(), use the right lr and beta1\n",
    "optimizerD = torch.optim.Adam(netD.parameters(), lr=lrD)\n",
    "# rajouter optimizer avec momentum modifié \n",
    "# optimizerD = torch.optim.Adam(netD.parameters(), lr=1e-4)\n",
    "# optimizerD = torch.optim.Adam(netD.parameters(), lr=1e-2)\n",
    "\n",
    "# optimizerG = torch.optim.Adam(netG.parameters(), lr=lrG, betas=(beta1G,0.999)) # same for G\n",
    "optimizerG = torch.optim.Adam(netG.parameters(), lr=lrG)\n",
    "# rajouter optimizer avec momentum modifié \n",
    "# optimizerG = torch.optim.Adam(netG.parameters(), lr=1e-4)\n",
    "# optimizerG = torch.optim.Adam(netG.parameters(), lr=1e-2)\n",
    "# Note that adam's betas is a tuple, set the second element of the tuple to 0.999 for both optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Otp9cFuJhVDA"
   },
   "outputs": [],
   "source": [
    "# Data format / batch creation functions\n",
    "\n",
    "fixed_noise = get_noise(196) # Create a fixed random vector sampled from a Gaussian, will be used during train for viz\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "iterator = iter(dataloader)\n",
    "\n",
    "# returns a batch of real images from the dataset (iterates infinitely on the dataset)\n",
    "def get_batch_real():\n",
    "    global iterator\n",
    "    try:\n",
    "        x_real = next(iterator)[0].to(device)\n",
    "    except:\n",
    "        iterator = iter(dataloader)\n",
    "        x_real = next(iterator)[0].to(device)\n",
    "    y_real = torch.full((x_real.size(0),), real_label, device=device)\n",
    "    return x_real, y_real\n",
    "\n",
    "# TODO\n",
    "# returns a batch of generated images and training targets y_fake\n",
    "# Note that the targets y_fake will be different is train_G is True or False\n",
    "def get_batch_fake(train_G=False):\n",
    "    z = get_noise(batch_size)\n",
    "    x_fake = netG(z) # TODO generate images from z\n",
    "    if(train_G):\n",
    "      y_fake = torch.ones((batch_size),requires_grad=False) \n",
    "    else:\n",
    "      y_fake = torch.zeros((batch_size),requires_grad=False) # TODO create targets, depends on train_G\n",
    "    return x_fake, y_fake.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bz7Pt6bDhVVH"
   },
   "outputs": [],
   "source": [
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SB5h3eyywZ5L"
   },
   "source": [
    "Donner la procédure d'entraînement de ces deux réseaux. L'optimisation se fera ADAM selon les deux coûts adverses du discriminateur et du générateur. Pour chaque nouveau batch d'images, on alterne les deux mises à jour suivantes, selon un batch de vecteurs $z$ tirés aléatoirement selon une loi normale centrée réduite (un nouveau batch de $z$ à chaque itération): \n",
    "\n",
    "1.   Un pas de gradient sur les paramètres du réseau D pour maximiser:  $log(D(x)) + log(1 - D(G(z)))$\n",
    "2.   Un pas de gradient sur les paramètres du réseau G pour maximiser:  $log(D(G(z)))$ \n",
    "\n",
    "\n",
    "Afin de suivre l'évolution de l'apprentissage, on pourra logguer l'erreur du discriminateur relevée en 1, l'erreur du générateur relevée en 2, la moyenne des sorties du discriminateur sur les images réelles et la moyenne des sorties du discriminateur sur les images générées.\n",
    "\n",
    "À la fin de chaque époque (i.e., lorsque l'on a itéré sur tous les batchs du DataLoader), on pourra enregistrer les images générées à partir d'un batch de vecteurs $z$ fixe dans le Google Drive pour observer l'évolution des capacités du générateur.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "q_vKyuNn6COz",
    "outputId": "f02a4642-025a-4686-ff4c-4e76b4408a22"
   },
   "outputs": [],
   "source": [
    "for i in range(steps):\n",
    "    \n",
    "    ########\n",
    "    # Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "    for _ in range(nb_update_D):\n",
    "        netD.zero_grad()\n",
    "        \n",
    "        # Create batches\n",
    "        x_real, y_real = get_batch_real()\n",
    "        x_fake, y_fake = get_batch_fake()\n",
    "        \n",
    "        # Forward \n",
    "        y_hat_real = netD(x_real) # TODO\n",
    "        y_hat_fake = netD(x_fake) # TODO\n",
    "        errD = criterion(y_hat_real,y_real)+criterion(y_hat_fake,y_fake)       # TODO sum of criterion of real and fake samples\n",
    "        \n",
    "        # Backward\n",
    "        # TODO backward & optimization step on D\n",
    "        errD.backward()\n",
    "        optimizerD.step()\n",
    "        \n",
    "        # Compute / save metricsthe derivative for 'target' is not implemented\n",
    "        avg_output_for_real = y_hat_real.mean().item()\n",
    "        avg_output_for_fake = y_hat_fake.mean().item()    \n",
    "        D_losses.append(errD.item())\n",
    "      \n",
    "\n",
    "\n",
    "    ########\n",
    "    # Update G network: maximize log(D(G(z)))\n",
    "    for _ in range(nb_update_G):\n",
    "        netG.zero_grad()\n",
    "        \n",
    "        # TODO: forward + backward\n",
    "        # NOTE: use errG as name for your loss variable, like errD above\n",
    "        x_fake, y_fake = get_batch_fake(train_G=True)\n",
    "        \n",
    "        y_hat_fake = netD(x_fake)\n",
    "        errG = criterion(y_hat_fake,y_fake)\n",
    "        # errG = torch.mean(torch.log(1 - y_hat_fake)) \n",
    "\n",
    "        errG.backward()\n",
    "        optimizerG.step()\n",
    "        # Compute / save metrics\n",
    "        G_losses.append(errG.item())\n",
    "        \n",
    "    \n",
    "    ########\n",
    "    # Logs\n",
    "    if i % 25 == 0:\n",
    "        print('[%5d/%5d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f'\n",
    "              % (i, steps, errD.item(), errG.item(), avg_output_for_real, avg_output_for_fake))\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        with torch.no_grad():\n",
    "            x_fake = netG(fixed_noise).detach().cpu()\n",
    "        img_list.append(vutils.make_grid(x_fake, padding=2, normalize=True, nrow=14))\n",
    "        plt.figure(figsize=(8,8))\n",
    "        plt.imshow(np.transpose(img_list[-1],(1,2,0)))\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Loss evolution\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.subplot(2,1,1)\n",
    "plt.title(\"Generator Training Loss\")\n",
    "plt.plot(G_losses)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.title(\"Discriminator Training Loss\")\n",
    "plt.plot(D_losses)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HPKl9QI1_7fZ"
   },
   "source": [
    "Le code ci-dessous applique votre réseau à un batch de $z$ aléatoires et affiche les images générées (et enregistre dans fake.png). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 862
    },
    "colab_type": "code",
    "id": "UFbL5Es00IEL",
    "outputId": "54fad49b-77e1-42f1-e19d-eef1acc41f39"
   },
   "outputs": [],
   "source": [
    "noise = torch.randn(64, nz, 1, 1, device=device)\n",
    "with torch.no_grad():\n",
    "  netG.eval()\n",
    "  fake = netG(noise).detach().cpu()\n",
    "img=vutils.make_grid(fake, padding=2, normalize=True)\n",
    "img_list.append(img)\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Fake Images\")\n",
    "plt.imshow(np.transpose(img.cpu(),(1,2,0)))\n",
    "plt.savefig( \"fake.png\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zqb9k8hAUxFN"
   },
   "source": [
    "Le générateur du papier original DCGAN possède en fait l'architecture suivante: \n",
    "\n",
    "![Generator](https://pytorch.org/tutorials/_images/dcgan_generator.png)\n",
    "\n",
    "Comme le réseau $G$ définit plus haut, il correspond à un empilement de couches de convolutions transposées (appelée dans certains papiers couches de déconvolution). Contrairement aux convolutions classiques qui mènent à une réduction de la taille des sorties, les convolutions transposées agrandissent les cartes de caractéristiques considérées (feature maps). C'est particulièrement adapté pour de la génération d'images à partir d'un code de petite taille (ici $z$). \n",
    "\n",
    " Pour comprendre comment fonctionne la convolution transposée, voici un exemple simple avec une entrée 4 x 4 et un noyau 4 x 4. Chaque élément de l'entrée (4 éléments) est multiplié par le noyau et le résultat est ajouté à la sorte de taille 3 x 3: \n",
    "\n",
    "![BasicTransposeConv2D](https://d2l.ai/_images/trans_conv.svg)\n",
    "\n",
    "Et voici deux animations pour se représenter l'opération d'une manière plus générale. A gauche on utilise un stride de 1, à droite un stride de 2: \n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://i.stack.imgur.com/YyCu2.gif\">\n",
    "<img src=\"https://i.stack.imgur.com/f2RiP.gif\">\n",
    "</p>\n",
    "\n",
    "\n",
    "Suivant la doc Pytorch de torch.nn.ConvTranspose2d, la hauteur $H_{out}$ et la largeur $W_{out}$ des cartes de sortie du ConvTranspose2d peuvent se calculer de la manière suivante: \n",
    "\n",
    "$H_{out}$=($H_{in}$−1)×stride−2×padding+dilation×(kernel_size−1)+output_padding+1\n",
    "\n",
    "$W_{out}$=($W_{in}$−1)×stride−2×padding+dilation×(kernel_size−1)+output_padding+1\n",
    "\n",
    "\n",
    "Proposer un nouveau réseau $G$ qui respecte l'architecture du schéma du papier DCGAN et comparer les résultats. On gardera le paramètre de dilation à sa valeur de 1 par défaut mais il est possible de moduler les valeurs de padding et output_padding pour obtenir des sorties de la taille désirée. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DTa7CDT5mEFY"
   },
   "outputs": [],
   "source": [
    "ndf = 32 # Base size of feature maps in discriminator\n",
    "ngf = 32 # Base size of feature maps in generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XxyfZLVQ6EgJ"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d( nz, 1024, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(1024, 512, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d( 256, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d( 128, 3, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "netG = Generator().to(device)\n",
    "netG.apply(weights_init)\n",
    "print(netG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SvbeOuRJEYZR"
   },
   "source": [
    "Bonus: Réaliser le même genre d'apprentissage sur le corpus Mnist (Dataloader existant dans torch pour télécharger et charger le corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OfAjhYVR6FMy"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TME9_RLD.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
